from .modules import SelfAttentionWide, SelfAttentionWide, TransformerBlock, CustomTransformerBlock, CustomSelfAttentionNarrow 

from .transformers import GTransformer, CTransformer, CustomTransformer

from .my_transformer_model import my_multihead_attention_mechanism